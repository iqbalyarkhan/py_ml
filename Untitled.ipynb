{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ng Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis and Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a training set is fed to a learning algorithm, it generates an output function called <b> hypothesis </b> denoted by $h$. The job of hypothesis function is to take input, say the size of a house, and output its price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, $h$ maps from $x$ to $y$ where $x$ is the independent variable (size of the house) and $y$ is the dependent variable (price of the house)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to represent the hypothesis $h$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_{\\theta}(x)$ = ${\\theta}_0 + {\\theta}_1.x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that $y$ or $h_{\\theta}(x)$ is a linear function. This is called <b> Univariate Linear Regression </b> (or single variable linear regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Linear Regression and Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>m</b>: Number of training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this equation: $h_{\\theta}(x)$ = ${\\theta}_0 + {\\theta}_1.x$, the ${\\theta}_is$ are called <b>parameters</b> of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${\\theta}_0$ and ${\\theta}_1$ help determine what the cost function would look like. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say for example if ${\\theta}_0$ = 1.5 and ${\\theta}_1$ = 0, we'll get a straight horizontal line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the idea is to get the most accurate ${\\theta}_0$ and ${\\theta}_1$ values so as to minimize the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to choose ${\\theta}_0$ and ${\\theta}_1$ so that $h_{\\theta}(x)$ is close to $y$ for our training examples $(x,y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want $$ J({\\theta_0,\\theta_1}) = \\frac{1}{2m} \\sum _{i=1}^m \\left(h_\\theta(X^{(i)})-Y^{(i)}\\right)^2$$ (difference between the output of my hypothesis and the actual price of the house squared) to be as small as possible. $m$ here is the number of training examples. Multiplying it by half to make math easier and the $m$ is present to get average over all the training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $J({\\theta_0,\\theta_1})$ is the <b> squared error cost function </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to minimize this cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example with cost function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say our cost function is like so where ${\\theta}_0 = 0$ and ${\\theta}_1$ is the slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$h_{\\theta}(x) = {\\theta}_1.x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are two key functions we're intersted in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(a)$ The hypothesis function $h_{\\theta}(x)$ (which is a function of $x$ which is the size of the house) and\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(b)$ The cost function $J({\\theta_1})$ (which controls the slope of our line through the points) is a function of our parameter ${\\theta_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $h_{\\theta}(x)$ will go through points $(1,1) (2,2) (3,3)$ and so on if I choose ${\\theta_1}$ as $1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what will the value of $J({\\theta_1})$ (the cost function) be, if ${\\theta_1} = 1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, our cost function was: $$ J({\\theta_0,\\theta_1}) = \\frac{1}{2m} \\sum _{i=1}^m \\left(h_\\theta(X^{(i)})-Y^{(i)}\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where in our case ${\\theta_0}$ will be $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, since our ${\\theta_1} = 1$, the value inside parenthesis will be 0 (since, predicted value = exact value), therefore, cost function = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, $J(1) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each value of ${\\theta_1}$, these were the values of $J({\\theta_1})$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>${\\theta_1} = 1$ , $J({\\theta_1}) = 0$ </li>\n",
    "<li> ${\\theta_1} = 0.5$ , $J({\\theta_1}) = 0.583$</li>\n",
    "<li> ${\\theta_1} = 0$ , $J({\\theta_1}) = 2.3$</li>\n",
    "<li> ${\\theta_1} = -0.5$ , $J({\\theta_1}) = 5.25$</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want to choose the value of ${\\theta_1}$ that minimizes $J({\\theta_1})$, which is the first set in the list above: ${\\theta_1} = 1$ , $J({\\theta_1}) = 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example with cost function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous example was for a cost function where ${\\theta}_0 = 0$ and ${\\theta}_1$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have have a cost function where both thetas are non-zero, we get a cost function that is bowl shaped. It is also 3D. AKA contour plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Gradient Descent </b> is used to minimize the cost function. The ideas is that we start with some ${\\theta}_0$ and ${\\theta}_1$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is similar to a person going toward the lowest point in the valley by taking small steps to points that'll take you down the fastest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent formula: ${\\theta}_j := {\\theta}_j - {\\alpha}\\frac{\\partial}{\\partial {\\theta}_j}J({\\theta_0,\\theta_1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few things to note here:\n",
    "<li> ${\\alpha}$ is the learning rate ie how fast you want to descend </li>\n",
    "<li> $\\frac{\\partial}{\\partial {\\theta}_j}J({\\theta_0,\\theta_1})$ is the derivative that tells you which direction to move in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating data for our linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAG+5JREFUeJzt3X2sHNd93vHvr6Fki6opSyaNqJau\naTmG0tjwmy77Yjipb+kkkpxEL0EECg5rNSpuzTSB0zYmzAqXbSm0jOjQMdq0DIRYcBIIlFlXTF2j\nQqP4rquithxeGnqhUsvviuUalmwldl0FVpT8+sfM6s7du3d3dvbMzJkzzwdY7O7s7M7Z2Zlnzp6Z\nOWPujoiIdN9fa7sAIiIShgJdRCQRCnQRkUQo0EVEEqFAFxFJhAJdRCQRCnQRkUQo0EVEEqFAFxFJ\nxLYmJ7Zz507fvXt3k5MUEem8s2fPfsvdd00br9FA3717N2tra01OUkSk88zsiTLjqclFRCQRCnQR\nkUQo0EVEEqFAFxFJhAJdRCQRCnQRkbocOwaDwcZhg0E2vAYKdBGRuuzZAzfdtB7qg0H2fM+eWibX\n6HHoIiK9srQEp05lIX7gAJw4kT1fWqplcqqhi4jUaWkpC/Pbb8/uawpzKBHoZnaXmT1lZufGvPbP\nzczNbGc9xRMR6bjBIKuZr6xk96Nt6gGVqaF/GLh6dKCZXQ78BPAngcskIpKGYZv5qVNw5Mh680tN\noT410N39AeCZMS/9BnAQ8NCFEhFJwpkzG9vMh23qZ87UMrlKO0XN7Drg6+7+sJkFLpKISCIOHtw8\nbGmptnb0mQPdzLYD/4KsuaXM+MvAMsDCwsKskxMRkZKqHOXyauBVwMNm9lXgMuCzZvaD40Z29zvd\nfdHdF3ftmtqdr4iIVDRzDd3dHwVePnyeh/qiu38rYLlERGRGZQ5bPAl8GrjSzJ40s1vrL5aIiMxq\nag3d3W+e8vruYKUREZHKdKaoiEgiFOgiIolQoIuIJEKBLiKSCAW6iEgiFOgiIolQoIuIJEKBLiKS\nCAW6iEgiFOgiIolQoIuIJEKBLiKSCAW6iEgiFOgiIolQoIuIJEKBLiKSCAW6iEgiFOgiIokoc03R\nu8zsKTM7Vxh2u5k9YmYPmdkfmNnfqLeYIiIyTZka+oeBq0eGvd/dX+/ubwQ+DhwOXTARkRccOwaD\nwcZhg0E2XF4wNdDd/QHgmZFh3y08vRDwwOUSEVm3Zw/cdNN6qA8G2fM9e9otV2Qqt6Gb2b8xs68B\n72RCDd3Mls1szczWnn766aqTE5E+W1qCU6eyED98OLs/dSobPovEa/qVA93db3P3y4G7gV+aMN6d\n7r7o7ou7du2qOjkR6bulJThwAG6/PbufNcyh2Zp+CxuPEEe53A38bIDPERHZ2mAAJ07Aykp2PxqW\n0wyDtFjTv/56uPHGahuHaVpoJqoU6Gb2msLT64DPhSmOiMgYwzA8dQqOHFkP5VlCfRiwsF7Tf/55\n2LevnjKHaiaawbZpI5jZSeBtwE4zexL4l8C1ZnYl8FfAE8C7ayuhiMiZMxvDcBiWZ86UD8jhe264\nAZ57DrZvh21TI3A+xWailZVawxzA3Js7QGVxcdHX1tYam56IyAaDAbzjHfDnf74esHXWnIf/LA4c\nyJqJKk7HzM66++K08XSmqIj0xz33wPnnr7fDw3pNP7QQzUQzUqCLSD8MBnDvvXD69MaABTh4MPz0\nJjUT1URNLiLSD8eOZTtGi00eg0EWsHUEekBlm1wU6CIikVMbuohIzyjQRUQSoUAXEUmEAl1EJBEK\ndBGRRCjQRUQSoUAXkY0S7zM8ZQp0EdlIVwfqLAW6SFHqtdMy36+Fbl8lDAW6SFEdtdOYNhJlv1+I\nqwNJ89y9sdtVV13lItFbXXXfudN9ZSW7X10N83nDzxl93rQy3y/0POiCO+7Y/D1XV7PhLQPWvETG\nKtBFxllZyVaPlZUwnxdbQE76frFtgJoS8fdWoItUVVf4ht5IVDXt+0VcU61dbBveXLBAB+4CngLO\nFYa9n+w6oo8Ap4GXlpmYAl2iV1ctLZagiLgWWkkdG59YNrwFZQO9zE7RDwNXjwy7H3idu78e+Dxw\naO7GfJEY1HFRghauXLOlFi66UKvQO7EHg+xKRsMrGrXxG82jTOoDuynU0EdeuwG4u8znqIYuvdTn\nJowmhPr3M8u/l4Z/UwLW0Kf5BeC+AJ8jkqaDBzcf9re0FP1Vcjoj1CGWs/x7ifXkqzKpzxY1dOA2\nsjZ0m/DeZWANWFtYWKhl6yUiPdbW/okGdy5Tdw3dzG4Bfgp4Zz7BrTYYd7r7orsv7tq1q+rkREQ2\na3P/xLR/Bi3U4isFupldDRwEfsbdnw1bJBGRktrcyTttB2oLXShMDXQzOwl8GrjSzJ40s1uB3wRe\nAtxvZg+Z2W/VVkIRkXGOHctqu8WAHAyyMK97/0TZfwYNd6EwNdDd/WZ3v9Tdz3P3y9z9Q+7+Q+5+\nubu/Mb+9u9ZSioiManPHZNl/Bg0fBmkTmr+DW1xc9LW1tcamJyKJG4b4gQNZYMbUK2SxFr+0tPn5\nDMzsrLsvThtPvS2KSHfF3Cvk+98Phw5trMUfOpQNr4kCXUS6K4YzO7fqHvnyy+Ho0Y1NQkePwnvf\nW1tRFOgi0k2xdKmwVVv+vn3xHeUiIhKlWPqlmXR4YmxHuYiIRKnpLhUmXXlqq+BuuElIgS4iUsak\nwyTHBXcLTUIKdElXTNfylO7bqmkFxgf3Pfc03iSkQJd0xdojXqr6sAEd17SyVVv+q1/dfC+bZXrw\nCnVTf+jSuFiuFNQHbV0Nqcm+yVtantA1RaUVMV7MIcJLiiWrjcBrakPS4uX7FOjSjtiuWakaevPa\n2IA28Tu3WFlRoEt7YgnR2DYufdDmb5/wP7Gyga6dohJeLP1rxHLiSV+EPkxvlp2sMXQBEIMyqR/q\nphp6T8RSQ5dmhW6SKPsPqwf/xChZQ1f3uRJWwC5DRV5Yft7wBlhbg9OnN56FOfy31daFLhqi7nOl\nHWrm6Ke6jkEfNt994hPw3HMbP3t4TkHTXQBETIEuYWnl6qdpJ3FVDfxi2/j558P11zfWc2EnTWuT\nAe4CngLOFYb9HPAY8FfAYpm2HVcbukjaJu07qdLOPe4927cneyTLJAQ8yuXDwNUjw84BNwIPhNms\niPRQaqfKTzq6aVIXs1sZbb4D2LYN9u7t95Esk5RJfWA3hRp6YfgnUQ1dYhbjmavFcqR0dEaZo5uq\nHive1LyKdHkh5IlFCvSOiHRhbFWZIGhzvqVyiGeZ+TzPd23qN4p0IxtNoAPLwBqwtrCw0MiX761I\nF8bWTQuSpufbaDgNa61799YzvSZMC9wuLZsRbmSjCfTiTTX0BkS4MEZh2l/9JudbMcxWV90vusj9\ngguy+1R/r679e4ysGwEFel+MW1H2749qYWxd2bBuciVeXXXfsSM7amPHjvVw10a4fRFWioIFOnAS\n+AbwF8CTwK3ADfnj7wPfBP57mYkp0GswGgLHj7ubZaEeycLYqllPH29yJd67d/MGJOZaax9E2jQU\ntIYe6qZAr8lwodu/Pwvz48c3Du9zqJf5q9/GShxhLVA82qYhBXrfDJsL9u/fODyChTF6d9zhvry8\n+YiM5eV6r3oTegMSaRjJ/MoGuk79T0Hx9Oj77tt4woVOu5/u4EHYt29zV6/33lvP9Ufr6u9G11CV\nMqkf6qYaeg0ibfPrpBSaQVL4DrIJqqH3hHo3DGeWC3PEetp+LBcXkVYo0LtOvRuGM8tVb2Jt3tCV\ne/qtTDU+1E1NLhKteXoDjKV5Q81vyUJNLiIzqNJ0FVvzRl3Nb7E2L8lmZVI/1E01dElKbDX0usTe\nwVkPoBq6SI1CX+E+ZmX6Mo91n0LPKNBFqujb0UXTmpeqXMBCglOgS3y60Gbbt6OLyhw9E9s+hR5S\noEt89Pc9LmWbl6aFfhc21B2nQJf49OHve5fCrUzzUpnQ14a6fmX2nIa66SgXcffyR0S0eZGBuo/a\nSO2Y8bLzqy9HBgWGeluUaNV9/cmmyhhqGn0Lt3k21D09PFKBLnGbFGazhOlwBS+u6MXn86zoTQRu\nZJc6q9288zS1fzYlKdAlfluF2Sy1sOEKffz4+PsyK/qk6dUZuH2roYcK477NNw8Y6MBdwFMUrikK\nXALcD3whv7+4zMQU6PKCkCvl6BWbZr383lZBM9wo1BEcfaxphmwu6dk/m5CB/mPAm0cC/Rjwvvzx\n+4A7ykxMgS7uXk+YDVfwH/3Raiv66AZmtIYfOnB72hYchGro8zW5ALtHAv1x4NL88aXA42U+R4Eu\n7h4+zOatoQ8Va30K3Dj18Z+N1x/of1Z4bMXnk24KdAkuRBt68XN6VOvrpJ5uaMsG+rYAx7G7mflW\nr5vZMrAMsLCwMO/kRDYanvRSPPnlTW/aOHzaCUnFk2KWlrJbiiczpWBc1wrD30ywLPynjGS2G/i4\nu78uf/448DZ3/4aZXQp80t2vnPY5i4uLvra2Nl+JpVuOHcvOBCyucINBFrSx9HvShTJKr5nZWXdf\nnDZe1VP/Pwa8K3/8LuC/VPwcSd08p3s3dXp83zrakmRNDXQzOwl8GrjSzJ40s1uBXwN+3My+ALw9\nfy6y2Tz9sqjvD5GZTA10d7/Z3S919/Pc/TJ3/5C7f9vd97r7a9z97e7+TBOFlY6q0q3qsBZe3Bjc\ncAPceOPs7aVd6ghLZA7qbVHqdewYfOADG7tV/cAHpofpsHYO6xuD556DfftmL0PsNX1tcCQQBXpo\nWjk32rYNfvVX4dChrFvVQ4ey59umHGA1bKq5/no4fhwuuADOP79aGWLvjjf2DY50hgI9NK2cGz3/\nPPz6r8PRo1mYHj2aPX/++fLvf/bZbCNw+nT163bGfDWd2Dc40h1lDlYPdevNiUU6SWWzKn1vLC+7\n79ixcT5WPYmkC7/JcB7t379xeA9OnJHJUG+LLavaeVCKZ8ItL7tfdNFswRzyFO8unC4+2n3B8eMb\nh9dZ1hSXucQo0Nu0uup+4YWb+xQps5J0IXxmsbqahfmOHetBXny+lZAhE3tgjf7Gx4/P1yfNvNPv\n+jKXIAV6W0L0LdKF5oGyhmFa/E47dmS19ianXxRTmLuPL+P+/dX+4VWV0jKXIAV6U0ZXxjvuyMJ7\nOHz4N3r79tlWkhT7e27jO3Wx9tlWuKa4zCVCgd6UaYFRZSVJsbbU5nfq0vxsawPUpXnUQwr0Jm21\nMlRZSWKqUYZqrojhO3Wl9tlGE1EMv49MpEBv2mhgVF1J6lihq37mrN9hq+lcc0277diqfU7Whf0M\nPadAb9K4wIhpJZmnBjZLGDZV06tyEWnVPqXDFOhN6UpgzFNLnaW5oona8CzzPKYNq0hFCvSmdCkw\nmtpB20R7tZpRpEcU6LJRUztomwzaruzoHFWmEtClioLUToEu65raQdtk81OXa+hl5lNXmvKkEQp0\nWddUba+p6aQQdmU2SF3eaElQCvRU6K/3ZqnMkzJNRl1tVpKgGgl04D3AOeAx4Femja9AryBUbTSV\nEEyFaugyg9oDHXhdHubbgW3AHwI/NOk9CvSKQqzYKTRTpEJt6DKjsoE+zxWL/ibwGXd/1t2fB/4H\ncOMcnydbCXG1nb5cFacLlwA8c2bjvB/+NmfOzDaOyKgyqT/uRhbonwdeRlZL/zTw7ye9RzX0ikL+\n9U69TVY1W0kQDbWh3wqcBR4ATgAfHDPOMrAGrC0sLDTz7UOIpc05ZED1pU22L99TeqORQPeNwf1v\ngV+cNE6nauix1PTGbViWlzdfIGLaxiaF0+VnKVfq/0SkV5qqob88v18APge8dNL4nQp093hrelU2\nNiE7tGor8Mt+71h/N5GKmgr0/wn8MfAwsHfa+J0LdPd4a3p1h9akz2/z38u07x3LPyuRgBpvcilz\n61ygx17Tq3tjM+nz25w3k8oVa3ORyBwU6POKvabXZg19qM1rhMa6kRWpgQJ9XjHX9Ore2Cwvu+/Y\nsfHzd+zYuCO2jWCNfSMrUpOygT7PiUVpO3hw80k3S0vZ8LY1cdKJ2dbPB4P1E5OOHFk/YWn0hJ6i\nECf86GQbkcnKpH6oW6dq6H03qQZe5d+LatcilaEmlxa11VwTerqh28jV/i1SSdlAV5NLHfbs2dgE\nMWyi2LOnO9MdDODECVhZye5Hm0uqNKGE6JNGRLZWJvVD3XpTQ3dvrzbaVM+MVZpQVEMXqQQ1uUSg\nrZOS5p1u2aabWQJabegilSnQ29blGvosym48Yj4MVCRyCvQ2tVUbbXq6akIRaUTZQNdO0Tq0dbx0\nk9Otciy6iNTKsvBvxuLioq+trTU2PanRsWPZ0TPFI1UGg2zjEcPJVyIJMbOz7r44dTwFuohI3MoG\nuppcREQSkVagd+ECwfPqw3cUkUrSCvQmztBsO1DbOgs1NW3/jiI1SCvQh0d13HQTHD68fhTGmTPh\nVt62A3Wr76jT6GfT9u8oUocyxzZudQP+KfAYcA44Cbx40viNHYc+erJL6OOzYzj+OtZL43VJDL+j\nSAnUfWIR8ArgK8AF+fNTwC2T3lMp0Gc9w3CrlTT0yttmoCqIwtGGUTqgqUD/GnAJsA34OPATk95T\nKdBnqV1PGzfUyttmoKbYJ8pWG+1rrqm3uwBtGKUjag/0bBq8B/ge8DRw97TxKze5lF3xJtXmQ628\nbQdqin2ibDVPjx+vb163/TuKzKCJGvrFwCqwCzgP+H3g58eMtwysAWsLCwvVv9EstevR0Ftddb/o\novVrYs6z8qYYqDFoqqlsSL+jdEgTgf5zwIcKz/8B8B8nvaf2Gvro+MPxRi96PBxHK29cttpoq51b\neq6JQP/b+REu2wEDfgf45Unvqb0Nfdz7UmofTblW2XQNXaRDmmpD/9fA5/LDFn8PeNGk8Rs5yqUo\ntZpdqu2+bbShi3RII4E+663R/tBD7Eitqs6adIo11raOchHpiH4HeshDHeuefhWp/fMQkYn6Heih\nTkaaR1016RRr6CIyUb8DvYo6ar2hPzPVNnQRmahsoKfVOVdVgwGcOAErK9l9iMuo1fGZbV3aTkS6\noUzqh7pFWUPvYhu6yKiUD2kV1dBLq6PWq5q0NE3dAQu6pqjEQBecDmMY4gcOZM186ic/GbqmaB+k\nctUd1S7DWFrKwvz227N7hXnvKNBDajpgUwlCXYUpjDp2xEu3lGloD3WLcqdoSG3sDE3puHSdMFWd\ndsQnDe0UbUEbNc1U/mZ3uXYZQ9OXdsQLRF5Db+NQrBDTbLKmmUINveu1y66XX6JHEmeKttmEUXWa\nTQZsKkGSwjHUKWxYJVppBLp7OytK1Wk2HbApBGFKtA9AapJOoLu3s6JUmaYCtr9UQ5capRPoXaqh\nh6SNQ3ek0vQl0Sob6HEf5TI8rvrUKThyZP0IkjqPgGhjmuOkcox5H+gIE4lE3Kf+t3FKeEynoetU\nbhGh/Kn/lQPdzK4EPlIYdAVw2N0/uNV71JdLBYcPZ8eYr6xk/xhEpHdq78vF3R939ze6+xuBq4Bn\ngdNVP0/G6PLJNiLSuFBt6HuBL7n7E4E+T2JpyxeRzggV6PuAk4E+S0A72kRkZnPvFDWz84H/A7zW\n3b855vVlYBlgYWHhqieeUCVeRGQWTfaHfg3w2XFhDuDud7r7orsv7tq1K8DkRERknBCBfjNqbhER\nad1cgW5mFwI/DtwbpjgiIlLVXIHu7v/P3V/m7t8JVaBaxdBvtYhITeI+9T80nU4vIgnb1nYBGlW8\nopBOpxeRxPSrhg7pXLJNRGRE/wJdp9OLSKL6Feg6nV5EEtavQNfp9CKSsLj7QxcRkUZP/RcRkQgo\n0EVEEqFAFxFJhAJdRCQRCnQRkUQ0epSLmT0NzHKFi53At2oqzrxUtmpUtmpUtmpSKdsr3X3qBSUa\nDfRZmdlamUN12qCyVaOyVaOyVdO3sqnJRUQkEQp0EZFExB7od7ZdgAlUtmpUtmpUtmp6Vbao29BF\nRKS82GvoIiJSUiuBbmZXm9njZvZFM3vfmNdfZGYfyV//jJntLrx2KB/+uJn9ZAtl+2dm9sdm9oiZ\nfcLMXll47S/N7KH89rEWynaLmT1dKMM/Krz2LjP7Qn57Vwtl+41CuT5vZn9WeK3u+XaXmT1lZue2\neN3M7N/lZX/EzN5ceK3u+TatbO/My/SomX3KzN5QeO2r+fCHzCx4r3clyvY2M/tO4bc7XHht4vLQ\nQNneWyjXuXwZuyR/re75drmZDfKceMzM3jNmnHqWOXdv9Ab8APAl4ArgfOBh4EdGxvlF4Lfyx/uA\nj+SPfyQf/0XAq/LP+YGGy7YEbM8fHxiWLX/+vZbn2y3Ab4557yXAl/P7i/PHFzdZtpHxfxm4q4n5\nln/+jwFvBs5t8fq1wH2AAX8H+EwT861k2d4ynCZwzbBs+fOvAjtbnG9vAz4+7/JQR9lGxv1pYLXB\n+XYp8Ob88UuAz49ZV2tZ5tqoof8t4Ivu/mV3fw64B7huZJzrgN/JH38U2Gtmlg+/x92/7+5fAb6Y\nf15jZXP3gbs/mz99ELgs4PTnKtsEPwnc7+7PuPufAvcDV7dYtpuBkwGnP5G7PwA8M2GU64Df9cyD\nwEvN7FLqn29Ty+bun8qnDc0ub2Xm21bmWVbrKFvTy9s33P2z+eP/C/xv4BUjo9WyzLUR6K8AvlZ4\n/iSbv+wL47j788B3gJeVfG/dZSu6lWwrO/RiM1szswfN7PqA5ZqlbD+b/4X7qJldPuN76y4beRPV\nq4DVwuA651sZW5W/7vk2q9HlzYE/MLOzZrbcUpn+rpk9bGb3mdlr82HRzDcz204WiP+5MLix+WZZ\nc/GbgM+MvFTLMretSiEFzOzngUXg7xUGv9Ldv25mVwCrZvaou3+pwWL9V+Cku3/fzP4x2b+cv9/g\n9MvYB3zU3f+yMKzt+RY9M1siC/S3Fga/NZ9vLwfuN7PP5TXXpnyW7Lf7npldC/w+8JoGp1/GTwP/\ny92LtflG5puZ/XWyDcmvuPt3Q3/+OG3U0L8OXF54flk+bOw4ZrYNuAj4dsn31l02zOztwG3Az7j7\n94fD3f3r+f2XgU+SbZkbK5u7f7tQnt8Grir73rrLVrCPkb+/Nc+3MrYqf93zrRQzez3Z73mdu397\nOLww354CThO2+XEqd/+uu38vf/zfgPPMbCeRzLfcpOWttvlmZueRhfnd7n7vmFHqWebq2jEwYYfB\nNrKG/lexvsPktSPj/BM27hQ9lT9+LRt3in6ZsDtFy5TtTWQ7fF4zMvxi4EX5453AFwi4I6hk2S4t\nPL4BeNDXd7R8JS/jxfnjS5osWz7eD5PtkLKm5lthOrvZeufeO9i4g+qPmphvJcu2QLav6C0jwy8E\nXlJ4/Cng6obL9oPD35IsFP8kn4elloc6y5a/fhFZO/uFTc63fB78LvDBCePUsswFncEzfOFryfb8\nfgm4LR92hKzGC/Bi4D/lC/IfAVcU3ntb/r7HgWtaKNsfAt8EHspvH8uHvwV4NF94HwVubaFsR4HH\n8jIMgB8uvPcX8vn5ReAfNl22/Pm/An5t5H1NzLeTwDeAvyBrk7wVeDfw7vx1A/5DXvZHgcUG59u0\nsv028KeF5W0tH35FPs8ezn/z21oo2y8VlrcHKWx0xi0PTZYtH+cWsoMoiu9rYr69layd/pHC73Zt\nE8uczhQVEUmEzhQVEUmEAl1EJBEKdBGRRCjQRUQSoUAXEUmEAl1EJBEKdBGRRCjQRUQS8f8BWH29\npYdOzQgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = 2 * np.random.rand(100,1)\n",
    "y = 8 + 2 * X + np.random.randn(100,1)\n",
    "plt.plot(X,y,\"rx\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal equation: $\\hat\\theta=(X^TX)^{-1}X^Ty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating $\\hat\\theta$ using the normal equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inserting $x_0$ as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.c_[np.ones((100,1)), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transposed = X.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_transposed.dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linalg.inv(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat = X.dot(X_transposed).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As the equation below shows, the normal equation found our ${\\theta_0}$ and ${\\theta_1}$ to be: 8.02 and 2.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_theta(x) = [7.83450398] + [2.13115028]x per normal equation\n"
     ]
    }
   ],
   "source": [
    "print(\"h_theta(x) = {0} + {1}x per normal equation\".format(theta_hat[0], theta_hat[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding ${\\theta_0}$ and ${\\theta_1}$ using gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_data():\n",
    "    X = 2 * np.random.rand(100,1)\n",
    "    y = 8 + 2 * X + np.random.randn(100,1)\n",
    "    return X,y\n",
    "\n",
    "\n",
    "def normal_equation(X,y):\n",
    "    # adding x0 = 1\n",
    "    X = np.c_[np.ones((100,1)), X]\n",
    "    X_transposed = X.transpose()\n",
    "    # print(\"shape of X is: {0}\".format(X.shape))\n",
    "    # print(\"shape of X` is: {0}\".format(X_transposed.shape))\n",
    "    X = X_transposed.dot(X)\n",
    "    X = np.linalg.inv(X)\n",
    "    theta_hat = X.dot(X_transposed).dot(y)\n",
    "    # print(\"Shape of y is: {0}\".format(y.shape))\n",
    "    # print(\"Actual values of y are: {0}\".format(y))\n",
    "    return theta_hat\n",
    "\n",
    "\n",
    "def gradient_descent(X,y):\n",
    "    alpha = 0.1\n",
    "    iterations = 1000\n",
    "    m = 100\n",
    "    theta = np.random.randn(2,1)\n",
    "    X = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "    for iteration in range(iterations):\n",
    "        X_transposed = X.transpose()\n",
    "        val = X_transposed.dot(X.dot(theta) - y)\n",
    "        theta = theta - (alpha/m)*(val)\n",
    "\n",
    "    return theta\n",
    "\n",
    "def main():\n",
    "    X, y = create_data()\n",
    "    theta_hat = normal_equation(X,y)\n",
    "    print(\"h_theta(x) = {0} + {1}x per normal equation\".format(theta_hat[0], theta_hat[1]))\n",
    "    theta_gd = gradient_descent(X,y)\n",
    "    print(\"theta_gd is: {0}\".format(theta_gd))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
